{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08843315-3609-4c67-b1f3-29922d30ed85",
   "metadata": {},
   "source": [
    "## Visualize outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2f59d4-2cfb-4a40-9562-3d54f5c87d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c27a4a-2cd5-4e55-ac51-848259cb2d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,inspect\n",
    "sys.path.insert(0,\"..\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "rc('font',**{'family':'sans-serif','sans-serif':['Computer Modern Roman']})\n",
    "rc('text', usetex=True)\n",
    "\n",
    "# Font Size\n",
    "# import matplotlib\n",
    "# font = {'family' : 'DejaVu Sans',\n",
    "#         'weight' : 'bold',\n",
    "#         'size'   : 12}\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdce517-31de-477f-8708-ce4f6e61876d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(idx, **images):\n",
    "    \"\"\"Plot images in one row.\"\"\" \n",
    "    n = len(images)\n",
    "    fig = plt.figure(figsize=(18, 16))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        #if idx==0:\n",
    "        plt.title(' '.join(name.split('_')).lower(), fontsize=20)\n",
    "        w,h = (1,25)\n",
    "        fs = 1.0\n",
    "        color = (0,0,0)\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX #FONT_HERSHEY_DUPLEX  #press tab for different operations\n",
    "        cv2.putText(image, str(idx), (w,h), font, fs, color, 1, cv2.LINE_AA)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(\"../outputs/analysis/{}.png\".format(idx), facecolor=\"white\", bbox_inches = 'tight')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def make_dataset(dir):\n",
    "    images = []\n",
    "    assert os.path.isdir(dir), '%s is not a valid directory' % dir\n",
    "\n",
    "    f = dir.split('/')[-1].split('_')[-1]\n",
    "    #print (dir, f)\n",
    "    dirs= os.listdir(dir)\n",
    "    for img in dirs:\n",
    "\n",
    "        path = os.path.join(dir, img)\n",
    "        #print(path)\n",
    "        images.append(path)\n",
    "    return images\n",
    "\n",
    "def read_image(path):\n",
    "    image = cv2.imread(path, -1)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59226d7-9286-4b5e-952c-fa9d8b3ea656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec3f342-be68-4dcf-9d41-06a16b424b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953451ff-eb1e-4cf2-91c4-4246c85f1d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to model\n",
    "algo1 = \"acgpn_5000\"\n",
    "algo2 = \"resunet_g1\"\n",
    "\n",
    "# Path to output tryon images\n",
    "mode = \"all\" # all, all_same, easy, medium, hard\n",
    "algo1_tryon_path = \"../outputs/{}/{}/\".format(algo1, mode)\n",
    "algo2_tryon_path = \"../outputs/{}/{}/\".format(algo2, mode)\n",
    "\n",
    "# File paths\n",
    "algo1_tryon_files = sorted(make_dataset(algo1_tryon_path))\n",
    "algo2_tryon_files = sorted(make_dataset(algo2_tryon_path))\n",
    "algo3_tryon_files = sorted(make_dataset(\"../outputs/cpvtonpp5000-try-ons\"))\n",
    "#algo4_tryon_files = sorted(make_dataset(\"../outputs/cpvtonpp-try-ons\"))\n",
    "\n",
    "# Path to warped clothes mask\n",
    "gt_wcr_path = \"../outputs/{}/{}/\".format(algo2, \"gt_clothes_mask\")\n",
    "algo1_wcr_path = \"../outputs/{}/{}/\".format(algo1, \"wcrmask\")\n",
    "algo2_wcr_path = \"../outputs/{}/{}/\".format(algo2, \"wcrmask\")\n",
    "\n",
    "# File paths\n",
    "gt_wcr_files = sorted(make_dataset(gt_wcr_path)) # gt mask\n",
    "algo1_wcr_path = sorted(make_dataset(algo1_wcr_path)) # algo1 pred\n",
    "algo2_wcr_path = sorted(make_dataset(algo2_wcr_path)) # algo2 pred\n",
    "\n",
    "# Path to reference person and cloth images\n",
    "ref_person_path = \"../../datasets/acgpn_data/try_on_testing/\"\n",
    "\n",
    "persons = []\n",
    "clothes = []\n",
    "\n",
    "with open(os.path.join(\"../../datasets/acgpn_data/try_on_testing/test_pairs.txt\"), 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        h_name, c_name = line.strip().split()\n",
    "        persons.append(h_name)\n",
    "        clothes.append(c_name)\n",
    "\n",
    "ref_person_paths = [os.path.join(ref_person_path, \"test_img\", x) for x in persons]\n",
    "target_clothes_paths = [os.path.join(ref_person_path, \"test_color\", x) for x in clothes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c7cb13-3650-43f0-a6d9-489cfb6fc183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc6b1cad-7f43-43e2-bca5-9447408e97db",
   "metadata": {},
   "source": [
    "### Show all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aab8325-f33a-45a8-adcc-0b17d37f1bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# for num in range(2032):\n",
    "#     visualize(i, reference_person=read_image(ref_person_paths[num]), target_clothes=read_image(target_clothes_paths[num]),\n",
    "#               Clothing_Region_GT=read_image(gt_wcr_files[num])[:,:,0], \n",
    "#               Unet=read_image(algo1_wcr_path[num])[:,:,0], resunet=read_image(algo2_wcr_path[num])[:,:,0],\n",
    "#               ACGPN=read_image(algo1_tryon_files[num]), Ours=read_image(algo2_tryon_files[num]))\n",
    "#     i+=1\n",
    "\n",
    "\n",
    "# Using this....\n",
    "# i = 0\n",
    "# for num in range(2032):\n",
    "#     visualize(i, reference_person=read_image(ref_person_paths[num]), target_clothes=read_image(target_clothes_paths[num]),\n",
    "#               ACGPN=read_image(algo1_tryon_files[num]), Ours=read_image(algo2_tryon_files[num]))\n",
    "#     i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e0a91a-f99f-4413-86f8-8694185809fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dc9ff5-3622-4b49-bd51-8c0e8480e6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for num in range(2032):\n",
    "    visualize(i, reference_person=read_image(ref_person_paths[num]), target_clothes=read_image(target_clothes_paths[num]),\n",
    "              ACGPN=read_image(algo1_tryon_files[num]), \n",
    "             CPVTONPP=read_image(algo3_tryon_files[num]),\n",
    "             Ours=read_image(algo2_tryon_files[num]))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48d796e-a198-4dc7-9881-c1997b5cb6b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5624612f-6340-4d32-96de-97855949fee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba96ba8c-df17-4b6f-a2e9-fa5019510715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddcee4c9-eb42-4ed7-83c4-c0887720061c",
   "metadata": {},
   "source": [
    "### Show masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2148065-9304-46ba-9d6b-9a654de52eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nums = np.array([6, 82, 17, 18, 23, 28, 57, 59, 67, 71, 120, 169, 239, 305, 307, 507, 1868, 1961, 1981, 2012])[-12:]\n",
    "# nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad3541d-9c6c-46b7-8e49-364c8a5450c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# for num in nums:\n",
    "#     visualize(i, reference_person=read_image(ref_person_paths[num]),\n",
    "#               Clothing_Region_GT=read_image(gt_wcr_files[num])[:,:,0], \n",
    "#               Unet=read_image(algo1_wcr_path[num])[:,:,0], resunet=read_image(algo2_wcr_path[num])[:,:,0])\n",
    "#     i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057779cf-8348-476a-a9bf-cf07da2ee9a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfc9e0d-50ad-4ffe-9e97-524795c680ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot using npstack\n",
    "\n",
    "# arrs = []\n",
    "# for refp, clt, gtwcr, a1wcr, a2wcr, a1, a2 in zip(ref_person_paths[:slc], target_clothes_paths[:3], \n",
    "#                   gt_wcr_files[:slc], algo1_wcr_path[:slc], algo2_wcr_path[:slc], algo1_tryon_files[:slc],\n",
    "#                   algo2_tryon_files[:slc]):\n",
    "    \n",
    "#     person=read_image(refp)\n",
    "#     cloth=read_image(clt)\n",
    "\n",
    "#     wcr=read_image(gtwcr) \n",
    "#     #wcr= np.where(wcr==127, 0, wcr)\n",
    "\n",
    "#     wcr_acgpn=read_image(a1wcr) \n",
    "#     #wcr_acgpn= np.where(wcr_acgpn==127, 0, wcr_acgpn)\n",
    "\n",
    "#     wcr_ours=read_image(a2wcr)\n",
    "#     #wcr_ours= np.where(wcr_ours==127, 0, wcr_ours)\n",
    "\n",
    "#     acgpn=read_image(a1)\n",
    "#     ours=read_image(a2)\n",
    "    \n",
    "#     out = np.hstack((person, cloth, wcr, wcr_acgpn, wcr_ours, acgpn, ours))\n",
    "#     #out = np.pad(out, pad_width=1, mode='constant', constant_values=0)\n",
    "#     arrs.append(out)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4318b380-5c32-48fc-b498-caff539de600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_output = np.vstack((x for x in arrs))\n",
    "# final_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ac83dd-b464-4cee-b6e7-ee9908441843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(18, 16))\n",
    "# plt.axis(\"off\")\n",
    "# plt.imshow(final_output, cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35681b17-f34c-415d-993b-1e992ec3bbe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8850ec2-8244-451e-b115-575860e55268",
   "metadata": {},
   "source": [
    "### Plot images with mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd00da3d-afda-4953-92cd-9ad593a1ea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nums = np.random.randint(0, 2032, 5)\n",
    "# nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcbe64f-0dc7-4931-ba0c-f5278982fa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nums = [i for i in range(5)] # 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b9e854-682c-4daa-9832-ca8d11e999ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows = len(nums)\n",
    "# cols = 7\n",
    "# fig, axarr = plt.subplots(rows, cols, figsize=(40, 38), constrained_layout=True)\n",
    "\n",
    "# for r in range(rows):\n",
    "#     reference_person=read_image(ref_person_paths[nums[r]])\n",
    "#     target_clothes=read_image(target_clothes_paths[nums[r]])\n",
    "#     Clothing_Region_GT=read_image(gt_wcr_files[nums[r]])[:,:,0]\n",
    "#     Unet=read_image(algo1_wcr_path[nums[r]])[:,:,0]\n",
    "#     resunet=read_image(algo2_wcr_path[nums[r]])[:,:,0]\n",
    "#     ACGPN=read_image(algo1_tryon_files[nums[r]])\n",
    "#     Ours=read_image(algo2_tryon_files[nums[r]])\n",
    "    \n",
    "#     images = [reference_person, target_clothes, Clothing_Region_GT, Unet, resunet, ACGPN, Ours]\n",
    "#     captions = [\"Reference \\n Person\", \"Target \\n Clothes\", \"Clothing Region \\n Ground Truth\", \"U-Net\", \"Res U-Net\", \"ACGPN\", \"Ours\"]\n",
    "    \n",
    "#     for c in range(cols):\n",
    "#         axarr[r, c].imshow(images[c], cmap='gray')\n",
    "#         axarr[r, c].axis(\"off\")\n",
    "#         axarr[r, c].set_aspect('equal')\n",
    "#         if r==0:\n",
    "#             axarr[r, c].set_title(captions[c], fontsize=50)\n",
    "\n",
    "# plt.savefig(\"../outputs/visualization_all.png\", facecolor=\"white\", bbox_inches = 'tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b955beb5-249f-46e8-ac11-e12ab7d9afe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05c1eedc-5323-43de-acce-40d84914dbe2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tryon images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f5064b-4438-4ba5-9d7c-33cd87ac091e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = np.random.randint(0, 2032, 12)\n",
    "nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5c86bc-1243-41ad-87be-5cc4aded31a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# acgpn and res acgpn\n",
    "#nums = np.array([6, 82, 17, 18, 23, 28, 57, 59, 67, 71, 120, 169, 239, 305, 307, 507, 1868, 1961, 1981, 2012])[-12:]\n",
    "\n",
    "# With acgpn, cpvton+, res acgpn\n",
    "#nums = np.array([5,6,9,18,59,172,189,190,275,277,322,566,743,1024,1417,1954,2012, 2031])[:12]\n",
    "nums = np.array([2031,2012,9,18,743,1417,189,190,275,277,322,566,743,1024,1417,1954,2012, 2031])[:12]\n",
    "nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568599e2-f65d-4b06-b016-8d1f941afc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d1ba5d-cf07-41fe-8ab6-9f6e9d938306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nums = [i for i in range(12)]\n",
    "# nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979ebe8f-74b3-4e71-b8c8-d0ab1928957c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = int(len(nums) / 3)\n",
    "cols = 15\n",
    "fig, axarr = plt.subplots(rows, cols, figsize=(40, 15), constrained_layout=True)\n",
    "\n",
    "\n",
    "alphabet_string = string.ascii_lowercase\n",
    "alphabet_list = list(alphabet_string)\n",
    "\n",
    "v = 0\n",
    "for r in range(rows):\n",
    "    rp1=read_image(ref_person_paths[nums[v+r]])\n",
    "    w,h = (1,25)\n",
    "    fs = 1.0\n",
    "    color = (0,0,0)\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX #FONT_HERSHEY_DUPLEX  #press tab for different operations\n",
    "    cv2.putText(rp1, str(alphabet_list[v+r]), (w,h), font, fs, color, 1, cv2.LINE_AA)\n",
    "    tc1=read_image(target_clothes_paths[nums[v+r]])\n",
    "    a1=read_image(algo1_tryon_files[nums[v+r]])\n",
    "    cpv1 = read_image(algo3_tryon_files[nums[v+r]])\n",
    "    o1=read_image(algo2_tryon_files[nums[v+r]])\n",
    "    \n",
    "    rp2=read_image(ref_person_paths[nums[v+r+1]])\n",
    "    cv2.putText(rp2, str(alphabet_list[v+r+1]), (w,h), font, fs, color, 1, cv2.LINE_AA)\n",
    "    tc2=read_image(target_clothes_paths[nums[v+r+1]])\n",
    "    a2=read_image(algo1_tryon_files[nums[v+r+1]])\n",
    "    cpv2 = read_image(algo3_tryon_files[nums[v+r+1]])\n",
    "    o2=read_image(algo2_tryon_files[nums[v+r+1]])\n",
    "    \n",
    "    rp3=read_image(ref_person_paths[nums[v+r+2]])\n",
    "    cv2.putText(rp3, str(alphabet_list[v+r+2]), (w,h), font, fs, color, 1, cv2.LINE_AA)\n",
    "    tc3=read_image(target_clothes_paths[nums[v+r+2]])\n",
    "    a3=read_image(algo1_tryon_files[nums[v+r+2]])\n",
    "    cpv3 = read_image(algo3_tryon_files[nums[v+r+2]])\n",
    "    o3=read_image(algo2_tryon_files[nums[v+r+2]])\n",
    "    \n",
    "    v+=2\n",
    "    \n",
    "    images = [rp1, tc1, a1, cpv1, o1, rp2, tc2, a2, cpv2, o2, rp3, tc3, a3, cpv3, o3]\n",
    "    \n",
    "    captions = [\"Reference \\n Person\", \"Target \\n Clothes\", \"ACGPN\", \"CP-VTON+\", \"Ours\",\n",
    "                \"Reference \\n Person\", \"Target \\n Clothes\", \"ACGPN\", \"CP-VTON+\", \"Ours\",\n",
    "                \"Reference \\n Person\", \"Target \\n Clothes\", \"ACGPN\", \"CP-VTON+\", \"Ours\"]\n",
    "    \n",
    "    for c in range(cols):\n",
    "        axarr[r, c].imshow(images[c], cmap='gray')\n",
    "        axarr[r, c].axis(\"off\")\n",
    "        axarr[r, c].set_aspect('equal') \n",
    "        if r==0:\n",
    "            axarr[r, c].set_title(captions[c], fontsize=40)\n",
    "\n",
    "plt.savefig(\"../outputs/visualization_tryons.png\", facecolor=\"white\", bbox_inches = 'tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c6e12f-5353-4d68-b9e5-554d700924bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cacb636-7a84-4b31-905a-bde4bc306259",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
